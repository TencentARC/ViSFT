model_config:
  lorra: &lorra
    model_data_dir: ${env.data_dir}
    losses:
    - type: logit_bce
    num_context_features: 1
    context_feature_dim: 300
    image_feature_dim: 2048
    context_max_len: 50
    classifier:
      type: logit
      params:
        img_hidden_dim: 5000
        text_hidden_dim: 300
    image_feature_embeddings:
    - modal_combine:
        type: non_linear_element_multiply
        params:
          dropout: 0
          hidden_dim: 5000
      normalization: softmax
      transform:
        type: linear
        params:
          out_dim: 1
    context_feature_embeddings:
    - modal_combine:
        type: non_linear_element_multiply
        params:
          dropout: 0
          hidden_dim: 5000
      normalization: sigmoid
      transform:
        type: linear
        params:
          out_dim: 1
    image_feature_encodings:
    - type: finetune_faster_rcnn_fpn_fc7
      params:
        bias_file: models/detectron.defaults/fc7_b.pkl
        weights_file: models/detectron.defaults/fc7_w.pkl
        model_data_dir: ${model_config.lorra.model_data_dir}
    - type: default
      params:
        model_data_dir: ${model_config.lorra.model_data_dir}
    context_feature_encodings:
    - type: default
      params:
        model_data_dir: ${model_config.lorra.model_data_dir}
    image_text_modal_combine:
      type: non_linear_element_multiply
      params:
        dropout: 0
        hidden_dim: 5000
        # 300 for FastText and 50 for order vectors
        context_dim: 350
    text_embeddings:
    - type: attention
      params:
        hidden_dim: 1024
        num_layers: 1
        conv1_out: 512
        conv2_out: 2
        dropout: 0
        embedding_dim: 300
        kernel_size: 1
        padding: 0
        model_data_dir: ${model_config.lorra.model_data_dir}
    context_embeddings:
    - type: identity
      params:
        embedding_dim: 350
        model_data_dir: ${model_config.lorra.model_data_dir}

  lorra_with_glove: *lorra
