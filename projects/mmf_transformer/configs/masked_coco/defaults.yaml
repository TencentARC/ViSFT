model_config:
  mmf_transformer:
    modalities:
      - type: text
        key: text
        position_dim: 512
        segment_id: 0
        embedding_dim: 768
        layer_norm_eps: 1e-12
        hidden_dropout_prob: 0.1
      - type: image
        key: image
        embedding_dim: 2048
        position_dim: 1
        segment_id: 1
        layer_norm_eps: 1e-12
        hidden_dropout_prob: 0.1
        encoder:
            type: identity
            params:
                in_dim : 2048
    heads:
      - type: mlm
        freeze: false
        lr_multiplier: 1.0
        # default for bert base
        hidden_size: 768
        # default vocab size for bert base
        vocab_size: 30522

dataset_config:
  masked_coco:
    return_features_info: true

optimizer:
  type: adam_w
  params:
    lr: 5e-5
    eps: 1e-8

scheduler:
  type: warmup_linear
  params:
    num_warmup_steps: 1000
    num_training_steps: 11000

training:
  batch_size: 480
  lr_scheduler: true
  # Don't forget to update schedule_attributes if you update this
  max_updates: 11000
